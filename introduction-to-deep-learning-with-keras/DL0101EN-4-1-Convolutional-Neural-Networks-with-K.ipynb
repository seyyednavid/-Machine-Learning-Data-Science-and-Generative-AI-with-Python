{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "# Convolutional Neural Networks with Keras\n",
    "\n",
    "Estimated time needed **30** mins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn how to use the Keras library to build convolutional neural networks. We will also use the popular MNIST dataset and we will compare our results to using a conventional neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives for this Notebook    \n",
    "* How to use the Keras library to build convolutional neural networks\n",
    "* Convolutional neural network with one set of convolutional and pooling layers\n",
    "* Convolutional neural network with two sets of convolutional and pooling layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "      \n",
    "1. <a href=\"#item41\">Import Keras and Packages</a>   \n",
    "2. <a href=\"#item42\">Convolutional Neural Network with One Set of Convolutional and Pooling Layers</a>  \n",
    "3. <a href=\"#item43\">Convolutional Neural Network with Two Sets of Convolutional and Pooling Layers</a>  \n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item41'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.0.2\n",
      "  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-2.0.2\n",
      "Collecting pandas==2.2.2\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas==2.2.2) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas==2.2.2) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas==2.2.2) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 tzdata-2024.2\n",
      "Collecting tensorflow_cpu==2.18.0\n",
      "  Downloading tensorflow_cpu-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow_cpu==2.18.0) (24.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow_cpu==2.18.0) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow_cpu==2.18.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow_cpu==2.18.0) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow_cpu==2.18.0) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow_cpu==2.18.0) (2.0.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow_cpu==2.18.0)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow_cpu==2.18.0) (0.43.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow_cpu==2.18.0)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow_cpu==2.18.0)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow_cpu==2.18.0)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.18.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.18.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.18.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow_cpu==2.18.0) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow_cpu==2.18.0)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow_cpu==2.18.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow_cpu==2.18.0)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow_cpu==2.18.0) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow_cpu==2.18.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow_cpu==2.18.0) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow_cpu==2.18.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow_cpu-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.2/230.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.68.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.8/391.8 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow_cpu\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 keras-3.7.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.13.1 protobuf-5.29.2 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-io-gcs-filesystem-0.37.1 tensorflow_cpu-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.0\n",
      "Collecting matplotlib==3.9.2\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.9.2)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.9.2)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.9.2)\n",
      "  Downloading fonttools-4.55.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib==3.9.2)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib==3.9.2)\n",
      "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.9.2)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0 pyparsing-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. \n",
    "# If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.\n",
    "\n",
    "!pip install numpy==2.0.2\n",
    "!pip install pandas==2.2.2\n",
    "!pip install tensorflow_cpu==2.18.0\n",
    "!pip install matplotlib==3.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 21:54:51.321325: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-19 21:54:51.371670: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with convolutional neural networks in particular, we will need additional packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D # to add convolutional layers\n",
    "from keras.layers import MaxPooling2D # to add pooling layers\n",
    "from keras.layers import Flatten # to flatten data for fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item42'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with One Set of Convolutional and Pooling Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the pixel values to be between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  3.]\n",
      "  [ 18.]\n",
      "  [ 18.]\n",
      "  [ 18.]\n",
      "  [126.]\n",
      "  [136.]\n",
      "  [175.]\n",
      "  [ 26.]\n",
      "  [166.]\n",
      "  [255.]\n",
      "  [247.]\n",
      "  [127.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 30.]\n",
      "  [ 36.]\n",
      "  [ 94.]\n",
      "  [154.]\n",
      "  [170.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [225.]\n",
      "  [172.]\n",
      "  [253.]\n",
      "  [242.]\n",
      "  [195.]\n",
      "  [ 64.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 49.]\n",
      "  [238.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [251.]\n",
      "  [ 93.]\n",
      "  [ 82.]\n",
      "  [ 82.]\n",
      "  [ 56.]\n",
      "  [ 39.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 18.]\n",
      "  [219.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [198.]\n",
      "  [182.]\n",
      "  [247.]\n",
      "  [241.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 80.]\n",
      "  [156.]\n",
      "  [107.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [205.]\n",
      "  [ 11.]\n",
      "  [  0.]\n",
      "  [ 43.]\n",
      "  [154.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 14.]\n",
      "  [  1.]\n",
      "  [154.]\n",
      "  [253.]\n",
      "  [ 90.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [139.]\n",
      "  [253.]\n",
      "  [190.]\n",
      "  [  2.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 11.]\n",
      "  [190.]\n",
      "  [253.]\n",
      "  [ 70.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 35.]\n",
      "  [241.]\n",
      "  [225.]\n",
      "  [160.]\n",
      "  [108.]\n",
      "  [  1.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 81.]\n",
      "  [240.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [119.]\n",
      "  [ 25.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 45.]\n",
      "  [186.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [150.]\n",
      "  [ 27.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 16.]\n",
      "  [ 93.]\n",
      "  [252.]\n",
      "  [253.]\n",
      "  [187.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [249.]\n",
      "  [253.]\n",
      "  [249.]\n",
      "  [ 64.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 46.]\n",
      "  [130.]\n",
      "  [183.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [207.]\n",
      "  [  2.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 39.]\n",
      "  [148.]\n",
      "  [229.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [250.]\n",
      "  [182.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 24.]\n",
      "  [114.]\n",
      "  [221.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [201.]\n",
      "  [ 78.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 23.]\n",
      "  [ 66.]\n",
      "  [213.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [198.]\n",
      "  [ 81.]\n",
      "  [  2.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 18.]\n",
      "  [171.]\n",
      "  [219.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [195.]\n",
      "  [ 80.]\n",
      "  [  9.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [ 55.]\n",
      "  [172.]\n",
      "  [226.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [244.]\n",
      "  [133.]\n",
      "  [ 11.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [136.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [253.]\n",
      "  [212.]\n",
      "  [135.]\n",
      "  [132.]\n",
      "  [ 16.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  [  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255 # normalize training data\n",
    "X_test = X_test / 255 # normalize test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's convert the target variable into binary categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "num_classes = y_test.shape[1] # number of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function that creates our model. Let's start with one set of convolutional and pooling layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(16, (5, 5), strides=(1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's call the function to create the model, and then let's train it and evaluate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 15s - 49ms/step - accuracy: 0.9232 - loss: 0.2712 - val_accuracy: 0.9710 - val_loss: 0.0965\n",
      "Epoch 2/10\n",
      "300/300 - 13s - 43ms/step - accuracy: 0.9764 - loss: 0.0822 - val_accuracy: 0.9783 - val_loss: 0.0689\n",
      "Epoch 3/10\n",
      "300/300 - 12s - 41ms/step - accuracy: 0.9819 - loss: 0.0593 - val_accuracy: 0.9849 - val_loss: 0.0486\n",
      "Epoch 4/10\n",
      "300/300 - 12s - 41ms/step - accuracy: 0.9862 - loss: 0.0454 - val_accuracy: 0.9858 - val_loss: 0.0431\n",
      "Epoch 5/10\n",
      "300/300 - 12s - 40ms/step - accuracy: 0.9886 - loss: 0.0364 - val_accuracy: 0.9875 - val_loss: 0.0375\n",
      "Epoch 6/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9908 - loss: 0.0304 - val_accuracy: 0.9851 - val_loss: 0.0448\n",
      "Epoch 7/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9922 - loss: 0.0256 - val_accuracy: 0.9867 - val_loss: 0.0402\n",
      "Epoch 8/10\n",
      "300/300 - 12s - 41ms/step - accuracy: 0.9936 - loss: 0.0216 - val_accuracy: 0.9869 - val_loss: 0.0407\n",
      "Epoch 9/10\n",
      "300/300 - 12s - 41ms/step - accuracy: 0.9945 - loss: 0.0179 - val_accuracy: 0.9883 - val_loss: 0.0366\n",
      "Epoch 10/10\n",
      "300/300 - 12s - 41ms/step - accuracy: 0.9959 - loss: 0.0139 - val_accuracy: 0.9884 - val_loss: 0.0372\n",
      "Accuracy: 0.9883999824523926 \n",
      " Error: 1.1600017547607422\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = convolutional_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item43'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network with Two Sets of Convolutional and Pooling Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine our convolutional model so that it has two convolutional and pooling layers instead of just one layer of each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "    \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(28, 28, 1)))\n",
    "    model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(8, (2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function to create our new convolutional neural network, and then let's train it and evaluate it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 14s - 47ms/step - accuracy: 0.8839 - loss: 0.4141 - val_accuracy: 0.9624 - val_loss: 0.1246\n",
      "Epoch 2/10\n",
      "300/300 - 12s - 42ms/step - accuracy: 0.9663 - loss: 0.1145 - val_accuracy: 0.9755 - val_loss: 0.0800\n",
      "Epoch 3/10\n",
      "300/300 - 21s - 69ms/step - accuracy: 0.9742 - loss: 0.0853 - val_accuracy: 0.9789 - val_loss: 0.0658\n",
      "Epoch 4/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9791 - loss: 0.0687 - val_accuracy: 0.9814 - val_loss: 0.0562\n",
      "Epoch 5/10\n",
      "300/300 - 12s - 41ms/step - accuracy: 0.9824 - loss: 0.0586 - val_accuracy: 0.9853 - val_loss: 0.0476\n",
      "Epoch 6/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9847 - loss: 0.0513 - val_accuracy: 0.9863 - val_loss: 0.0420\n",
      "Epoch 7/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9864 - loss: 0.0452 - val_accuracy: 0.9852 - val_loss: 0.0438\n",
      "Epoch 8/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9886 - loss: 0.0395 - val_accuracy: 0.9876 - val_loss: 0.0387\n",
      "Epoch 9/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9887 - loss: 0.0362 - val_accuracy: 0.9888 - val_loss: 0.0337\n",
      "Epoch 10/10\n",
      "300/300 - 13s - 42ms/step - accuracy: 0.9904 - loss: 0.0320 - val_accuracy: 0.9882 - val_loss: 0.0369\n",
      "Accuracy: 0.9882000088691711 \n",
      " Error: 1.1799991130828857\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = convolutional_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: {} \\n Error: {}\".format(scores[1], 100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Alex Aklson](https://www.linkedin.com/in/aklson/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2024-11-20  | 3.0  | Aman  |  Updated the library versions to current |\n",
    "| 2020-09-21  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://opensource.org/license/mit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "10de0f094f07faef2f2a834458aa125dc47afabb0bb6d2ed7dfab1f66db6fde4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
